# This is a dummy config for testing the benchmarking framework.

# See tools/run.sh for the env vars used in this config
results_dir: "${CONTAINER_RESULTS_DIR}"
artifacts_dir: "${CONTAINER_ARTIFACTS_DIR}"
datasets:
  - name: "cc"
    formats:
    - type: "json"
      path: "${CONTAINER_DATASETS_DIR}/sample/25de70f6c6a6.jsonl"
    - type: "parquet"
      path: "${CONTAINER_DATASETS_DIR}/sample/25de70f6c6a6.parquet"

default_timeout_s: 7200

# Optional sinks
sinks:
  - name: mlflow
    enabled: false
    tracking_uri: ${MLFLOW_TRACKING_URI}
    experiment: ray-curator-common-crawl
  - name: slack
    enabled: true
    webhook_url: ${SLACK_WEBHOOK_URL}
  - name: gdrive
    enabled: false
    drive_folder_id: ${GDRIVE_FOLDER_ID}
    service_account_file: ${GDRIVE_SERVICE_ACCOUNT_FILE}

# Whether to delete scratch dirs after each run
delete_scratch: true

entries:
  - name: demo
    script: dummy_benchmark.py
    args: >-
      --input-path {dataset:cc,parquet}
      --output-path {session_entry_dir}/scratch/output
    timeout_s: 20000
    ray:
      num_cpus: 4
      num_gpus: 0
      enable_object_spilling: false
  - name: demo2
    script: dummy_benchmark.py
    args: >-
      --input-path {dataset:cc,parquet}
      --output-path {session_entry_dir}/scratch/output
    timeout_s: 20000
    ray:
      num_cpus: 4
      num_gpus: 0
      enable_object_spilling: false
